# EE2211 20/21 S1

## Lecturer
Haizhou Li, Kar Ann Toh

## Overview of content

This module covers the basics of machine learning. It starts with a basic refresher on linear algebra and statistics, then moves on to basic data engineering techniques (e.g. encoding, validation, etc.) and fundemental supervised and unsupervised machine learning algorithms. Topics include things like polynomial regression and regulariization, clustering and k-means, tree-based models (e.g. random forest) and vanilla neural networks.

The content is pretty touch-and-go, but I think it serves as a decent overview of basic machine learning techniques. CS3244 covers more things like logistic regression, SVM and goes deeper into neural networks, so I guess this class is closer to CS2109S. 

## Grading and exams
3 (pretty easy) assignments, a midterm and a final. Pretty standard stuff, not too hard. 

## Workload 

Honestly, I think this is one of the easier modules - the content isn't very heavy, and it's paced reasonably well (in my opinion, could be faster, actually). The assessments are also very manageable - if you pay attention in lecture, keep up with tutorials, spend time on actually implementing the algorithms taught in class (it's recommended but not compulsory in the tutorials), you should manage fine. Like a 6/10. 

## How I studied for it

I can only speak for myself, because I loved the content of the module a lot - it was actually the reasonw why I changed my course of study to CS. I generally have a deep love for linear algebra, calculus and stats, and so I was really motivated to study outside of class, and read the textbooks for ME5401 (Deep Learning for robotics) and CS3244 (Machine Learning). I recommend it too, if you find yourself enjoying the content! 

I also spent a good amount of time implementing the algorithms taught in Jupyter Notebooks (I've attached them here - sorry for the code quality, though, I've gotten much better since). Specifically, I implement all the models taught from scratch as we were expected to do them by hand. I also implement other relevant calculations like calculating performance metrics, probability/statistics, etc. 

Exams wise - they're very computation heavy, and it's open book - so prepare a Jupyter Notebook to do calculations, and make sure you understand how to compute stuff. The first half (polynomial regression) and last part (neural networks) are very matrices-heavy, so just be aware of that going in, the middle stuff (trees, clustering, etc.) are more algorithms heavy, so just be careful and go step-by-step. 

## My thoughts

A pretty fun, low-stakes introduction to machine learning. I took the first iteration of the class, so it wasn't very "stable", but it was fun, the topics were interesting, though I wish the class went deeper. A 6/10 overall. 

## Final grade 
A+
